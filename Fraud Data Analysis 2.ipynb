{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     a = []\n",
    "#     for item in x:\n",
    "#         a.append(1/(1+math.exp(-item)))\n",
    "#     return a\n",
    "\n",
    "# x = np.arange(-8., 8., 0.2)\n",
    "# sig = sigmoid(x)\n",
    "\n",
    "# hline = np.full((80,1), 0.5)\n",
    "\n",
    "# plt.plot(x,sig, color=\"black\")\n",
    "# plt.plot(x, hline, color=\"blue\", linestyle = \"dashed\")\n",
    "\n",
    "# plt.xlim(-8,8)\n",
    "# #plt.hlines(0.5, xmin=-8, xmax=8, linestyles=\"dashed\", colors=\"red\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input & Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there are no null values within our dataset, this however is to be expected as a PCA transformation has already been applied for GDPR reasons, and thus a level of data cleansing has likely already taken place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in Fraud_df.columns:\n",
    "    Col_Null = Fraud_df[c].isnull().sum()\n",
    "    if Col_Null > 0:\n",
    "        print(str(c) + \": \" + str(Col_Null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that there were two transactions within 0 seconds of the first, one of which is necessarily the first. Over a 48 hour period with almost 284807 instances of card usage this is not unlikely. It is interesting to note however that 1825 transactions were for of $0.\n",
    "Finally, we can see that of 284807 transactions, 284315 were legitimate (99.83\\%) (Therefore 492 Fraudulent), this will be the crux of our machine learning problem, as such a high class imbalance has a potential to lead to false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Fraud_df == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), sharex=True)\n",
    "\n",
    "sns.countplot('Class', data=Fraud_df, ax=ax1)\n",
    "ax1.set_title('Class Imbalance', fontsize=14)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot('Class', data=Fraud_df, ax=ax2).set_yscale('log')\n",
    "ax2.set_title('Class Imbalance (Logarithmic)', fontsize=14)\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_ylim(ymin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fraud vs Spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it initially appears from this dataset that the average case of fraud is for lower value transactions, with a maximum stolen of little over €2,000, as opposed to the legitimate transaction's maximum of over €25,000\n",
    "\n",
    "Once we account for outliers, we can see that the median transaction amount for fraudulent transactions is lower than the median for legitimate transactions, although the deviation (IQR) is notably higher. The overall higher range of (non-anomalous) fraudulent cases however leads to a higher mean for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(wspace = 0.75)\n",
    "plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "s1 = sns.boxplot(data=Fraud_df.loc[Fraud_df['Class']==0], x=\"Class\", y=\"Amount\", color=\"Blue\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "s2 = sns.boxplot(data=Fraud_df.loc[Fraud_df['Class']==1], x=\"Class\", y=\"Amount\", color=\"Orange\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(data = Fraud_df, x = \"Class\", y = \"Amount\", palette=[\"Blue\", \"Orange\"])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(data = Fraud_df, x = \"Class\", y = \"Amount\", showmeans=True, meanline=True, showfliers=False, palette=[\"Blue\", \"Orange\"])\n",
    "\n",
    "plt.plot([], [], '-', linewidth=8, color='Blue', label='No Fraud')\n",
    "plt.plot([], [], '-', linewidth=8, color='Orange', label='Fraud')\n",
    "plt.plot([], [], '--', linewidth=1, color='Green', label='Mean')\n",
    "\n",
    "plt.legend()\n",
    "#plt.legend(labels = [\"No Fraud\", \"Fraud\", \"Mean\"], color = [\"blue\", \"orange\", \"green\"], ) \n",
    "#plt.legend(labels = [\"Mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legit_det = Fraud_df[Fraud_df[\"Class\"] == 0][[\"Time\", \"Amount\"]].describe()\n",
    "Fraud_det = Fraud_df[Fraud_df[\"Class\"] == 1][[\"Time\", \"Amount\"]].describe()\n",
    "\n",
    "display(Legit_det)\n",
    "display(Fraud_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fraud vs Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately due to the confidentiality requirements of the original dataset we are unable to view the objective times of card transactions, however we are able to see timings relative to the first spend of the dataset. This means that while we may not know the hour of day transactions occurred, we can see trends in spending across the two day data gathering period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.kdeplot(Fraud_df[Fraud_df[\"Class\"] == 0][\"Time\"], label=\"No Fraud\")\n",
    "g = sns.kdeplot(Fraud_df[Fraud_df[\"Class\"] == 1][\"Time\"], label=\"Fraud\")\n",
    "#g.xlabel(\"Time\")\n",
    "g.set(yticks=[0.00000114285, 0.00000228571, 0.00000342857, 0.00000457142, 0.00000571428, 0.00000685714, 0.000008])\n",
    "g.set_yticklabels([2000, 4000, 6000, 8000, 10000, 12000, 14000])\n",
    "g.set_xlim(-10000, 200000)\n",
    "g.set_ylabel(\"Count\")\n",
    "g.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = sns.displot(Fraud_df, x=Fraud_df[Fraud_df[\"Class\"] == 0][\"Time\"], binwidth=5000)\n",
    "t1.fig.set_size_inches(12,4)\n",
    "#plt.xlim(0, 172800)\n",
    "\n",
    "t2 = sns.displot(Fraud_df, x=Fraud_df[Fraud_df[\"Class\"] == 1][\"Time\"], binwidth=5000)\n",
    "t2.fig.set_size_inches(12,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.subplot(2,1,1)\n",
    "sns.distplot(Fraud_df, x=Fraud_df[Fraud_df[\"Class\"] == 0][\"Time\"], bins = 34)\n",
    "plt.xlim(0, 180000)\n",
    "#plt.xlim(0, 175000)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(Fraud_df, x=Fraud_df[Fraud_df[\"Class\"] == 1][\"Time\"], bins = 34)\n",
    "plt.xlim(0, 180000)\n",
    "#plt.xlim(0, 175000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen here, there is a drop in legitimate transactions around 100,000 seconds after data collection began (~ 27 hours, 45 minutes). Fraudulent transactions, though dropping around 20,000 seconds later, or 5 and a half hours, do not drop as significantly. Given the data collection period length, we might assume that the 100,000 second drop corresponds with the early hours of the morning (It would be reasonable to start and end data collection at midnight on both days for record gathering simplicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we filter for variables with a correlation coefficient to transaction amount with absolute value above 0.25, we can see that V2 has a significant negative correlation, along with weaker negative correlations from V5.\n",
    "\n",
    "We have weak positive correlations from V7, and V20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df.corr()[\"Amount\"][Fraud_df.corr()[\"Amount\"].abs()>0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same for correlation with Fraud, we get only weak correlations, all negative, of V12, V14, and V17. When we create an algorithm to predict fraud we would therefore expect these to play a more heavily weighted part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df.corr()[\"Class\"][Fraud_df.corr()[\"Class\"].abs()>0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing Over/Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the class imbalance of our dataset if we use it without further refinement we will be subject to our algorithms overfitting the data, that is, tending towards the assumption that most transactions are not fraudulent, simply because this is more likely to be correct by chance.\n",
    "\n",
    "#### For Undersampling:\n",
    "\n",
    "We randomly select an equal number of legitimate transactions as there are fraudulent, giving us a dataset of 984 transactions, with a 50/50 split by class. The random sampling function also shuffles these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample dataset with a 50/50 split of fraudulent to genuine transactions\n",
    "No_Fraud_Under_Samp = Fraud_df[Fraud_df[\"Class\"] == 0].sample(n=492)\n",
    "Fraud_Under_Samp = Fraud_df[Fraud_df[\"Class\"] == 1].sample(n=492)\n",
    "\n",
    "Under_Samp_Fraud = pd.concat([Fraud_Under_Samp, No_Fraud_Under_Samp]).sample(frac=1)\n",
    "\n",
    "Under_Samp_Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Under_Genuine_No = Under_Samp_Fraud[Under_Samp_Fraud[\"Class\"] == 0][\"Class\"].count()\n",
    "Under_Fraud_No = Under_Samp_Fraud[Under_Samp_Fraud[\"Class\"] == 1][\"Class\"].count()\n",
    "\n",
    "print(Under_Genuine_No)\n",
    "print(Under_Fraud_No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Oversampling:\n",
    "\n",
    "We randomly select a number of fraudulent transactions (allowing for repeat selections) as there are genuine, giving us a dataset of 100,000 transactions, with a 1:20 fraud to genuin class split. The random sampling function also shuffles these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample dataset with 5% fraudulent transactions\n",
    "No_Fraud_Over_Samp = Fraud_df[Fraud_df[\"Class\"] == 0].sample(n=95000)\n",
    "Fraud_Over_Samp = Fraud_df[Fraud_df[\"Class\"] == 1].sample(n=5000, replace=True)\n",
    "\n",
    "Over_Samp_Fraud = pd.concat([Fraud_Over_Samp, No_Fraud_Over_Samp]).sample(frac=1)\n",
    "\n",
    "Over_Samp_Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Over_Genuine_No = Over_Samp_Fraud[Over_Samp_Fraud[\"Class\"] == 0][\"Class\"].count()\n",
    "Over_Fraud_No = Over_Samp_Fraud[Over_Samp_Fraud[\"Class\"] == 1][\"Class\"].count()\n",
    "\n",
    "print(Over_Genuine_No)\n",
    "print(Over_Fraud_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split our training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Under_Samp_Minus_Class = Under_Samp_Fraud.drop(\"Class\", axis=1)\n",
    "Under_Samp_Class = Under_Samp_Fraud[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into 30% training, 70% test group\n",
    "Input_train, Input_test, Res_train, Res_test = train_test_split(Under_Samp_Minus_Class, Under_Samp_Class, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "#Logistic Regression requires Array inputs\n",
    "Input_train = Input_train.values\n",
    "Input_test = Input_test.values\n",
    "Res_train = Res_train.values\n",
    "Res_test = Res_test.values\n",
    "\n",
    "print(\"Transactions in X_train dataset: \", Input_train.shape)\n",
    "print(\"Transaction classes in y_train dataset: \", Res_train.shape)\n",
    "\n",
    "print(\"Transactions in X_test dataset: \", Input_test.shape)\n",
    "print(\"Transaction classes in y_test dataset: \", Res_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(max_iter = 1000)\n",
    "LogReg.fit(Input_train, Res_train)\n",
    "\n",
    "LogReg_Train_Acc = accuracy_score(Res_train, LogReg.predict(Input_train))\n",
    "print(\"Classifiers: Logistic Regression has a Training Accuracy of\", 100*LogReg_Train_Acc, \"%\")\n",
    "\n",
    "LogReg_Cross_Val = cross_val_score(LogReg, Input_train, Res_train, cv=5)\n",
    "print(\"Classifiers: Logistic Regression has a CrossVal score of\", 100*LogReg_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising parameters for best fit using GridSearchCV. (This turns out to have only a minor effect on the accuracy, around 0.2% !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_Para = {\"penalty\": ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \"max_iter\": [500, 1000, 1500]}\n",
    "\n",
    "GridLogReg = GridSearchCV(LogisticRegression(), LogReg_Para)\n",
    "GridLogReg.fit(Input_train, Res_train)\n",
    "\n",
    "Log_Reg = GridLogReg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg_Train_Acc = accuracy_score(Res_train, Log_Reg.predict(Input_train))\n",
    "print(\"Classifiers: Logistic Regression has a Training Accuracy of\", 100*Log_Reg_Train_Acc, \"%\")\n",
    "\n",
    "Log_Reg_Cross_Val = cross_val_score(Log_Reg, Input_train, Res_train, cv=5)\n",
    "print(\"Classifiers: Logistic Regression has a CrossVal score of\", 100*Log_Reg_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(title, data, Samp_Minus_Class, Samp_Class, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10), verbose=0):\n",
    "    \n",
    "    _, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    \n",
    "#    if ylim is not None:\n",
    "#        axes[0].set_ylim(*ylim)\n",
    "#    axes[0].set_xlabel(\"Training dataset size\")\n",
    "#    axes[0].set_ylabel(\"Score\")\n",
    "    \n",
    "    \n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(data, Samp_Minus_Class, Samp_Class, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose = verbose, return_times=True)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    \n",
    "    \n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training Cross-Val score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Testing Cross-Val score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "    axes.set_title(title, fontsize=14)\n",
    "    \n",
    "    \n",
    "#    axes[0].grid()\n",
    "#    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                         color=\"r\")\n",
    "#    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "#                         color=\"g\")\n",
    "#    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#                 label=\"Training Cross-Val score\")\n",
    "#    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#                 label=\"Testing Cross-Val score\")\n",
    "#    axes[0].legend(loc=\"best\")\n",
    "#    axes[0].set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
    "    \n",
    "#    axes[1].grid()\n",
    "#    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "#    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "#                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "#    axes[1].set_xlabel(\"Training examples\")\n",
    "#    axes[1].set_ylabel(\"fit_times\")\n",
    "#    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    \n",
    "    \n",
    "#    axes[1].grid()\n",
    "#    a = sorted(list(zip(fit_times_mean, test_scores_mean)))\n",
    "#    axes[1].plot([x[0] for x in a], [x[1] for x in a], \"o-\")\n",
    "#    axes[1].plot(fit_times_mean, test_scores_mean, \"o-\")\n",
    "#    axes[1].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "#                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "#    axes[1].set_xlabel(\"fit_times\")\n",
    "#    axes[1].set_ylabel(\"Score\")\n",
    "#    axes[1].set_title(\"Performance of the model\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(\"Logistic Regression Learning Curve\", Log_Reg, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, when used to predict fraud in the entire dataset, our model is over 20x higher than the actual number of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1)).sum() / 492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_predicted, title):\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_predicted)\n",
    "    # Get the per-class normalized value for each cell\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # We color each cell according to its normalized value, annotate with exact counts.\n",
    "    ax = sns.heatmap(cm_norm, annot=cm, fmt=\"d\")\n",
    "    ax.set(xticklabels=[\"non-fraud\", \"fraud\"], yticklabels=[\"non-fraud\", \"fraud\"])\n",
    "    ax.set_ylim([0,2])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, Log_Reg.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, Log_Reg.predict(Input_train)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, Log_Reg.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, Log_Reg.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(label, prediction, train=True):\n",
    "    if train:\n",
    "#        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
    "        print(f\"Train Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
    "       \n",
    "    elif train==False:\n",
    "#        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
    "        print(f\"Test Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RanFor = RandomForestClassifier(n_estimators=100, oob_score=False)\n",
    "RanFor.fit(Input_train, Res_train)\n",
    "\n",
    "RanFor_Training_Acc = accuracy_score(Res_train, RanFor.predict(Input_train))\n",
    "RanFor_Cross_Val = cross_val_score(RanFor, Input_train, Res_train, cv=5)\n",
    "\n",
    "#RanFor_train_pred = RanFor.predict(Input_train)\n",
    "#RanFor_test_pred = RanFor.predict(Input_test)\n",
    "\n",
    "#print_score(Res_train, RanFor_train_pred, train=True)\n",
    "#print_score(Res_test, RanFor_test_pred, train=False)\n",
    "\n",
    "print(\"Classifiers: Random Forest has a Training Accuracy of\", 100*RanFor_Training_Acc, \"%\")\n",
    "print(\"Classifiers: Random Forest has a CrossVal score of\", 100*RanFor_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising using GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RanFor_Para = {\"bootstrap\": [True], \"max_features\": [5, 10, 15, 20, 25, 30], \"min_samples_leaf\": [1, 2, 3, 4], \\\n",
    "\"min_samples_split\": [2, 4, 6, 8, 10], \"n_estimators\": [10, 50, 100, 200, 300, 500]}\n",
    "\n",
    "GridRanFor = GridSearchCV(RandomForestClassifier(), RanFor_Para, cv = 5, n_jobs = -1, verbose = 5)\n",
    "\n",
    "GridRanFor.fit(Input_train, Res_train)\n",
    "\n",
    "Ran_For = GridRanFor.best_estimator_\n",
    "\n",
    "#RanFor_train_pred = RanFor.predict(Input_train)\n",
    "#RanFor_test_pred = RanFor.predict(Input_test)\n",
    "\n",
    "#print_score(Res_train, RanFor_train_pred, train=True)\n",
    "#print_score(Res_test, RanFor_test_pred, train=False)\n",
    "\n",
    "Ran_For_Training = accuracy_score(Res_train, Ran_For.predict(Input_train))\n",
    "Ran_For_Cross_Val = cross_val_score(Ran_For, Input_train, Res_train, cv=5)\n",
    "\n",
    "print(\"Classifiers: Random Forest has a Training Accuracy of\", 100*Ran_For_Training, \"%\")\n",
    "print(\"Classifiers: Random Forest has a CrossVal score of\", 100*Ran_For_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, when used to predict fraud in the entire dataset, our model is over 10x higher than the actual number of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ran_For.predict(Fraud_df.drop(\"Class\", axis=1)).sum() /492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RanFor.get_params())\n",
    "print(Ran_For.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RanFor_Cross_Val.mean() > Ran_For_Cross_Val.mean():\n",
    "    Ran_For = RanFor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(\"Random Forest Learning Curve\", Ran_For, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, Ran_For.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, Ran_For.predict(Input_train)).ravel()\n",
    "\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, Ran_For.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, Ran_For.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], Ran_For.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], Ran_For.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(Input_train, Res_train)\n",
    "\n",
    "LDA_Training_Acc = accuracy_score(Res_train, LDA.predict(Input_train))\n",
    "LDA_Cross_Val = cross_val_score(LDA, Input_train, Res_train, cv=5)\n",
    "\n",
    "print(\"Classifiers: LDA has a Training Accuracy of\", 100*LDA_Training_Acc, \"%\")\n",
    "print(\"Classifiers: LDA has a CrossVal score of\", 100*LDA_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.predict(Fraud_df.drop([\"Class\"], axis=1)).sum() /492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(\"Linear Discriminant Analysis Learning Curve\", LDA, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, LDA.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, LDA.predict(Input_train)).ravel()\n",
    "\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, LDA.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, LDA.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], LDA.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], LDA.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Over_Samp_Minus_Class = Over_Samp_Fraud.drop(\"Class\", axis=1)\n",
    "Over_Samp_Class = Over_Samp_Fraud[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into 30% training, 70% test group\n",
    "Input_train, Input_test, Res_train, Res_test = train_test_split(Over_Samp_Minus_Class, Over_Samp_Class, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "#Logistic Regression requires Array inputs\n",
    "Input_train = Input_train.values\n",
    "Input_test = Input_test.values\n",
    "Res_train = Res_train.values\n",
    "Res_test = Res_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(max_iter = 1000)\n",
    "LogReg.fit(Input_train, Res_train)\n",
    "\n",
    "LogReg_Train_Acc = accuracy_score(Res_train, LogReg.predict(Input_train))\n",
    "print(\"Classifiers: Logistic Regression has a Training Accuracy of\", 100*LogReg_Train_Acc, \"%\")\n",
    "\n",
    "LogReg_Cross_Val = cross_val_score(LogReg, Input_train, Res_train, cv=5)\n",
    "print(\"Classifiers: Logistic Regression has a CrossVal score of\", 100*LogReg_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising parameters for best fit using GridSearchCV. (This turns out to have only a minor effect on the accuracy, around 0.2% !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_Para = {\"penalty\": ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \"max_iter\": [500, 1000, 1500]}\n",
    "\n",
    "GridLogReg = GridSearchCV(LogisticRegression(), LogReg_Para, n_jobs = -1, verbose=10)\n",
    "GridLogReg.fit(Input_train, Res_train)\n",
    "\n",
    "Log_Reg = GridLogReg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg_Train_Acc = accuracy_score(Res_train, Log_Reg.predict(Input_train))\n",
    "print(\"Classifiers: Logistic Regression has a Training Accuracy of\", 100*Log_Reg_Train_Acc, \"%\")\n",
    "\n",
    "Log_Reg_Cross_Val = cross_val_score(Log_Reg, Input_train, Res_train, cv=5)\n",
    "print(\"Classifiers: Logistic Regression has a CrossVal score of\", 100*Log_Reg_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(title = \"Logistic Regression Learning Curve\", Log_Reg, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, when used to predict fraud in the entire dataset, our model is over twice as high as actual number of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1)).sum() / 492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_predicted, title):\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_predicted)\n",
    "    # Get the per-class normalized value for each cell\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # We color each cell according to its normalized value, annotate with exact counts.\n",
    "    ax = sns.heatmap(cm_norm, annot=cm, fmt=\"d\")\n",
    "    ax.set(xticklabels=[\"non-fraud\", \"fraud\"], yticklabels=[\"non-fraud\", \"fraud\"])\n",
    "    ax.set_ylim([0,2])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, Log_Reg.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, Log_Reg.predict(Input_train)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, Log_Reg.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, Log_Reg.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], Log_Reg.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(label, prediction, train=True):\n",
    "    if train:\n",
    "#        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
    "        print(f\"Train Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
    "       \n",
    "    elif train==False:\n",
    "#        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
    "        print(f\"Test Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RanFor = RandomForestClassifier(n_estimators=100, oob_score=False)\n",
    "RanFor.fit(Input_train, Res_train)\n",
    "\n",
    "RanFor_Training_Acc = accuracy_score(Res_train, RanFor.predict(Input_train))\n",
    "RanFor_Cross_Val = cross_val_score(RanFor, Input_train, Res_train, cv=5)\n",
    "\n",
    "#RanFor_train_pred = RanFor.predict(Input_train)\n",
    "#RanFor_test_pred = RanFor.predict(Input_test)\n",
    "\n",
    "#print_score(Res_train, RanFor_train_pred, train=True)\n",
    "#print_score(Res_test, RanFor_test_pred, train=False)\n",
    "\n",
    "print(\"Classifiers: Random Forest has a Training Accuracy of\", 100*RanFor_Training_Acc, \"%\")\n",
    "print(\"Classifiers: Random Forest has a CrossVal score of\", 100*RanFor_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we optimised via gridsearch, however due to the size of the oversampled dataset this is not practical here, also the accuracy of the default classifier seems as close to perfect as we can expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RanFor_Para = {\"bootstrap\": [True], \"max_features\": [5, 10, 15, 20, 25, 30], \"min_samples_leaf\": [1, 2, 3, 4], \\\n",
    "#\"min_samples_split\": [2, 4, 6, 8, 10], \"n_estimators\": [5, 10, 50, 100]}\n",
    "\n",
    "#GridRanFor = GridSearchCV(estimator = RanFor, param_grid = RanFor_Para, cv = 5, n_jobs = -1, verbose = 10)\n",
    "\n",
    "#GridRanFor.fit(Input_train, Res_train)\n",
    "\n",
    "#Ran_For = GridRanFor.best_estimator_\n",
    "\n",
    "#Ran_For_Cross_Val = cross_val_score(Ran_For, Input_train, Res_train, cv=5)\n",
    "#Ran_For_Training = accuracy_score(Res_train, Ran_For.predict(Input_train))\n",
    "\n",
    "#print(\"Classifiers: Random Forest has a Training Accuracy of\", 100*Ran_For_Training, \"%\")\n",
    "#print(\"Classifiers: Random Forest has a CrossVal score of\", 100*Ran_For_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ran_For = RanFor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used to predict fraud in the entire dataset, our model predicts relatively accurately the actual number of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ran_For.predict(Fraud_df.drop(\"Class\", axis=1)).sum() /492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(\"Random Forest Learning Curve\", Ran_For, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, Ran_For.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, Ran_For.predict(Input_train)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, Ran_For.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, Ran_For.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], Ran_For.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], Ran_For.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(Input_train, Res_train)\n",
    "\n",
    "LDA_Training_Acc = accuracy_score(Res_train, LDA.predict(Input_train))\n",
    "LDA_Cross_Val = cross_val_score(LDA, Input_train, Res_train, cv=5)\n",
    "\n",
    "print(\"Classifiers: LDA has a Training Accuracy of\", 100*LDA_Training_Acc, \"%\")\n",
    "print(\"Classifiers: LDA has a CrossVal score of\", 100*LDA_Cross_Val.mean(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used to predict fraud in the entire dataset, our model predicts almost perfectly the actual number of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.predict(Fraud_df.drop([\"Class\"], axis=1)).sum() /492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=42)\n",
    "plot_learning_curve(\"Linear Discriminant Analysis Learning Curve\", LDA, Input_train, Res_train, (0.85, 1.00), cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_train, LDA.predict(Input_train), \"Training Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_train, LDA.predict(Input_train)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Res_test, LDA.predict(Input_test), \"Testing Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Res_test, LDA.predict(Input_test)).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Fraud_df[\"Class\"], LDA.predict(Fraud_df.drop(\"Class\", axis=1)), \"Entire Dataset Matrix\")\n",
    "tn, fp, fn, tp = confusion_matrix(Fraud_df[\"Class\"], LDA.predict(Fraud_df.drop(\"Class\", axis=1))).ravel()\n",
    "\n",
    "print(\"Percentage of fraud missed: \", round(100*(fn / (fn+tp)), 2), \"%\")\n",
    "print(\"Percentage of genuine mis-classified: \", round(100*(fp / (fp+tn)), 2), \"%\")\n",
    "print(\"Total accuracy: \", round(100*((tp + tn)/(tp+tn+fp+fn)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing accuracy scores (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg_pred = cross_val_predict(Log_Reg, Input_train, Res_train, cv=5, method=\"decision_function\")\n",
    "\n",
    "Forest_pred = cross_val_predict(Ran_For, Input_train, Res_train, cv=5)\n",
    "\n",
    "LDA_pred = cross_val_predict(LDA, Input_train, Res_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ', roc_auc_score(Res_train, Log_Reg_pred))\n",
    "\n",
    "print('Decision Tree Classifier: ', roc_auc_score(Res_train, Forest_pred))\n",
    "\n",
    "print('LDA Classifier: ', roc_auc_score(Res_train, LDA_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source for below ROC graph: https://github.com/sxu11/Data_Science/blob/master/Anomaly_Detection/DealingWithImbalancedDataSets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_roc_curve_multiple(log_fpr, log_tpr, for_fpr, for_tpr, LDA_fpr, LDA_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(Res_train, Log_Reg_pred)))\n",
    "    plt.plot(for_fpr, for_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(Res_train, Forest_pred)))\n",
    "    plt.plot(LDA_fpr, LDA_tpr, label='Linear Discriminant Classifier Score: {:.4f}'.format(roc_auc_score(Res_train, LDA_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1.005])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_fpr, Log_tpr, Log_thresold = roc_curve(Res_train, Log_Reg_pred)\n",
    "Forest_fpr, Forest_tpr, Forest_threshold = roc_curve(Res_train, Forest_pred)\n",
    "LDA_fpr, LDA_tpr, LDA_thresold = roc_curve(Res_train, LDA_pred)\n",
    "    \n",
    "graph_roc_curve_multiple(Log_fpr, Log_tpr, Forest_fpr, Forest_tpr, LDA_fpr, LDA_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Reg_pred = cross_val_predict(Log_Reg, Fraud_df.drop(\"Class\", axis=1), Fraud_df[\"Class\"], n_jobs=-1, verbose=20, cv=5, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest_pred = cross_val_predict(Ran_For, Fraud_df.drop(\"Class\", axis=1), Fraud_df[\"Class\"], n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_pred = cross_val_predict(LDA, Fraud_df.drop(\"Class\", axis=1), Fraud_df[\"Class\"], n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression: ', roc_auc_score(Fraud_df[\"Class\"], Log_Reg_pred))\n",
    "\n",
    "print('Decision Tree Classifier: ', roc_auc_score(Fraud_df[\"Class\"], Forest_pred))\n",
    "\n",
    "print('LDA Classifier: ', roc_auc_score(Fraud_df[\"Class\"], LDA_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_roc_curve_full_dataset(log_fpr, log_tpr, for_fpr, for_tpr, LDA_fpr, LDA_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(Fraud_df[\"Class\"], Log_Reg_pred)))\n",
    "    plt.plot(for_fpr, for_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(Fraud_df[\"Class\"], Forest_pred)))\n",
    "    plt.plot(LDA_fpr, LDA_tpr, label='Linear Discriminant Classifier Score: {:.4f}'.format(roc_auc_score(Fraud_df[\"Class\"], LDA_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1.005])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_fpr, Log_tpr, Log_thresold = roc_curve(Fraud_df[\"Class\"], Log_Reg_pred)\n",
    "Forest_fpr, Forest_tpr, Forest_threshold = roc_curve(Fraud_df[\"Class\"], Forest_pred)\n",
    "LDA_fpr, LDA_tpr, LDA_thresold = roc_curve(Fraud_df[\"Class\"], LDA_pred)\n",
    "    \n",
    "graph_roc_curve_full_dataset(Log_fpr, Log_tpr, Forest_fpr, Forest_tpr, LDA_fpr, LDA_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
